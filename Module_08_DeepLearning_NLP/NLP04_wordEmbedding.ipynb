{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1557dbc9",
   "metadata": {},
   "source": [
    "## NLP04\n",
    "- 노드17 워드 임베딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7f7bc4",
   "metadata": {},
   "source": [
    "### 노드17 워드 임베딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef28117",
   "metadata": {},
   "source": [
    "#### 17-02 벡터화\n",
    "- Bag of words\n",
    "- DTM(Document-Term Matrix)\n",
    "- TF-IDF\n",
    "- 원-핫 인코딩(one-hot encoding)\n",
    "  - 고양이 [0,0,0,1..], 강아지 [1,0,0,0..] 같이 0과 1의 벡터로 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6eb542",
   "metadata": {},
   "source": [
    "#### 17-03 벡터화-실습\n",
    "- One-hot encoding 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8212a589",
   "metadata": {},
   "source": [
    "##### Step 1. 패키지 설치하기\n",
    "- LMS 환경에서는 설치되어 있음\n",
    "- pip install konlpy 명령어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d43170f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임포트 완료\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "print(\"임포트 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "678474e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'임금님 귀는 당나귀 귀! 임금님 귀는 당나귀 귀! 실컷~ 소리치고 나니 속이 확 뚫려 살 것 같았어.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"임금님 귀는 당나귀 귀! 임금님 귀는 당나귀 귀! 실컷~ 소리치고 나니 속이 확 뚫려 살 것 같았어.\"\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02625692",
   "metadata": {},
   "source": [
    "##### Step 2. 전처리\n",
    "- 정규 표현식을 사용하여 특수문자들을 제거\n",
    "  - 참고문서 : https://www.unicode.org/charts/PDF/U3130.pdf\n",
    "  - 참고문서2 : https://www.unicode.org/charts/PDF/UAC00.pdf\n",
    "- 한글, 공백을 제외한 모든 문자를 표현하는 regex : [^ㄱ-ㅎㅏ-ㅣ가-힣 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9e9b8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임금님 귀는 당나귀 귀 임금님 귀는 당나귀 귀 실컷 소리치고 나니 속이 확 뚫려 살 것 같았어\n"
     ]
    }
   ],
   "source": [
    "reg = re.compile(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\")\n",
    "text = reg.sub('', text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518a1dfa",
   "metadata": {},
   "source": [
    "##### Step 3. 토큰화 이야기\n",
    "- 참고(Okt) : https://konlpy.org/en/latest/api/konlpy.tag/#okt-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4f7d7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['임금님', '귀', '는', '당나귀', '귀', '임금님', '귀', '는', '당나귀', '귀', '실컷', '소리', '치고', '나니', '속이', '확', '뚫려', '살', '것', '같았어']\n"
     ]
    }
   ],
   "source": [
    "okt=Okt()\n",
    "tokens = okt.morphs(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b38a61",
   "metadata": {},
   "source": [
    "##### Step4. 단어장 만들기\n",
    "- 빈도수가 높은 단어일수록 낮은 정수를 부여\n",
    "- 파이썬의 Counter 서브클래스를 사용\n",
    "  - Collections.Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdb83d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'귀': 4, '임금님': 2, '는': 2, '당나귀': 2, '실컷': 1, '소리': 1, '치고': 1, '나니': 1, '속이': 1, '확': 1, '뚫려': 1, '살': 1, '것': 1, '같았어': 1})\n"
     ]
    }
   ],
   "source": [
    "vocab = Counter(tokens)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fa9794e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['임금님']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b03afdbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('귀', 4), ('임금님', 2), ('는', 2), ('당나귀', 2), ('실컷', 1)]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "vocab = vocab.most_common(vocab_size) # 등장 빈도수가 높은 상위 5개의 단어만 저장\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6be61ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'귀': 1, '임금님': 2, '는': 3, '당나귀': 4, '실컷': 5}\n"
     ]
    }
   ],
   "source": [
    "word2idx={word[0] : index+1 for index, word in enumerate(vocab)}\n",
    "print(word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d2a802",
   "metadata": {},
   "source": [
    "##### Step 5: 원-핫 벡터 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef906391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encoding(word, word2index):\n",
    "       one_hot_vector = [0]*(len(word2index))\n",
    "       index = word2index[word]\n",
    "       one_hot_vector[index-1] = 1\n",
    "       return one_hot_vector\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac12b713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoding(\"임금님\", word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aad5bc8",
   "metadata": {},
   "source": [
    "##### 케라스를 통한 원-핫 인코딩(one-hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0cbef9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750135130.488384     137 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750135130.502442     137 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750135130.552904     137 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750135130.552946     137 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750135130.552949     137 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750135130.552952     137 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임포트 완료\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "print(\"임포트 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1565b4e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['강아지', '고양이', '강아지'], ['애교', '고양이'], ['컴퓨터', '노트북']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [['강아지', '고양이', '강아지'],['애교', '고양이'], ['컴퓨터', '노트북']]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d93dd4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'강아지': 1, '고양이': 2, '애교': 3, '컴퓨터': 4, '노트북': 5}\n"
     ]
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(text)\n",
    "print(t.word_index) # 각 단어에 대한 인코딩 결과 출력."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66c6904e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(t.word_index) + 1\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62198470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 1, 4]]\n"
     ]
    }
   ],
   "source": [
    "sub_text = ['강아지', '고양이', '강아지', '컴퓨터']\n",
    "encoded = t.texts_to_sequences([sub_text])\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37fef7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "one_hot = to_categorical(encoded, num_classes = vocab_size)\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55317a41",
   "metadata": {},
   "source": [
    "#### 17-04 워드 임베딩\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8def6ecf",
   "metadata": {},
   "source": [
    "##### 희소 벡터(Sparse Vector)의 문제점\n",
    "- DTM, TF-IDF, 원-핫 벡터는 단어장의 크기에 영향을 받는 희소 벡터(sparse vector) 라는 특징\n",
    "- 차원의 저주(Curse of Dimensionality)\n",
    "- 기계가 단어 벡터 간 유사도를 구할 수 없다는 점은 오랫동안 자연어 처리의 걸림"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf24d3a",
   "metadata": {},
   "source": [
    "##### 워드 임베딩(Word Embedding)\n",
    "- 단어를 벡터로 바꾸지만, 벡터 길이를 일정하게 정해줌\n",
    "- 일반적으로 단어 길이 대비 벡터의 길이 매우 작으므로 dense하게 됨\n",
    "- 워드 임베딩은 2003년 요슈아 벤지오(Yoshua Bengio) 교수가 NPLM(Neural Probabilistic Language Model) 이란 모델\n",
    "- 2013년 구글은 NPLM을 개선하여 정밀도와 속도를 향상시킨 Word2Vec 을 제안\n",
    "- Word2Vec 이후로 FastText 나 GloVe 등과 같은 임베딩 방법이 추가로 제안"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23531c9",
   "metadata": {},
   "source": [
    "#### 17-05 Word2Vec (1) 분포 가설\n",
    "- Word2Vec의 핵심 아이디어는 분포 가설(distributional hypothesis) 을 따름\n",
    "- 이 가설은 언어학자 존 루퍼트 퍼스(John Rupert Firth)의 다음 인용 참고\n",
    "  - You shall know a word by the company it keeps (곁에 오는 단어들을 보면 그 단어를 알 수 있다)\n",
    "- 분포 가설 :‘비슷한 문맥에서 같이 등장하는 경향이 있는 단어들은 비슷한 의미를 가진다.’\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76abf5a8",
   "metadata": {},
   "source": [
    "#### 17-06 Word2Vec (2) CBoW\n",
    "- CBoW는 주변에 있는 단어들을 통해 중간에 있는 단어들을 예측하는 방법\n",
    "- Skip-Gram은 중간에 있는 단어로 주변 단어들을 예측하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b341a002",
   "metadata": {},
   "source": [
    "##### CBoW(Continuous Bag of words)\n",
    "- 예문 : \"I like natural language processing.\"\n",
    "  - 이때 예측해야 하는 단어 \"natural\"을 중심 단어(center word) 라고 하고, 예측에 사용되는 단어들을 주변 단어(context word) \n",
    "  - 중심 단어를 예측하기 위해 앞, 뒤로 몇 개의 단어를 볼지를 결정를 결정했다면, 그 범위를 윈도우(window) \n",
    "  - 윈도우 크기를 정했다면, 윈도우를 계속 움직여서 주변 단어와 중심 단어를 바꿔가며 학습을 위한 데이터 셋을 만들 수 있는데,    \n",
    "    이 방법을 슬라이딩 윈도우(sliding window) \n",
    "  - 이렇게 선택된 데이터셋에서 단어 각각은 원-핫 인코딩되어 원-핫 벡터가 되고, 원-핫 벡터가 CBoW나 Skip-gram의 입력\n",
    "-  Word2Vec은 은닉층이 1개라서 딥 러닝이라기보다는 얕은 신경망(Shallow Neural Network) 을 학습\n",
    "  - CBoW에서는 이 벡터들을 모두 합하거나, 평균을 구한 값 을 최종 은닉층의 결과\n",
    "  - Word2Vec에서는 은닉층에서 활성화 함수나 편향(bias)을 더하는 연산을 하지 않음\n",
    "  - Word2Vec에서의 은닉층은 활성화 함수가 존재하지 않고, 단순히 가중치 행렬과의 곱셈만을 수행하기에   \n",
    "    기존 신경망의 은닉층과 구분 지어 투사층(projection layer)라고도 부름"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e94bd8d",
   "metadata": {},
   "source": [
    "#### 17-07. Word2Vec (3) Skip-gram\n",
    "- Skip-gram과 Netative sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc04b030",
   "metadata": {},
   "source": [
    "##### Skip-gram\n",
    "- 주변 단어로 중심 단어를 예측하는 것이 아니라, 중심 단어로부터 주변 단어를 예측\n",
    "- 데이터셋의 형식은 (중심 단어, 주변 단어)임을 가정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8aee7c",
   "metadata": {},
   "source": [
    "##### 네거티브 샘플링(negative sampling)\n",
    "- Word2Vec를 사용할 때는 SGNS(Skip-Gram with Negative Sampling) 을 사용\n",
    "- Word2Vec의 구조는 연산량이 지나치게 많아 실제로 사용하기 어려움\n",
    "  - skipgram의 학습 과정은, 출력층에서 softmax함수를 통과한 V차원 벡터와 레이블 V차원 주변 단어의   \n",
    "    원-핫 벡터와의 오차를 구하고, 역전파를 통해 모든 단어에 대한 임베딩 벡터을 조정\n",
    "- 네거티브 샘플링은 연산량을 줄이기 위해서 소프트맥스 함수를 사용한 V개 중 1개를 고르는    \n",
    "  다중 클래스 분류 문제 를 시그모이드 함수를 사용한 이진 분류 문제 로 바꾸는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038c5411",
   "metadata": {},
   "source": [
    "#### 17-08. Word2Vec (4)\n",
    "- 영어 Word2Vec 실습과 OOV 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c08c37",
   "metadata": {},
   "source": [
    "##### 영어 Word2Vec 실습\n",
    "- cloud(LMS)에는 이미 설치되었으니 명령어만 참고\n",
    "  - pip install nltk\n",
    "  - pip install gensim\n",
    "- 참고(파라미터 의미)\n",
    "  - vector size = 학습 후 임베딩 벡터의 차원\n",
    "  - window = 컨텍스트 윈도우 크기\n",
    "  - min_count = 단어 최소 빈도수 제한 (빈도가 적은 단어들은 학습하지 않아요.)\n",
    "  - workers = 학습을 위한 프로세스 수\n",
    "  - sg = 0은 CBoW, 1은 Skip-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15290073",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package abc to /aiffel/nltk_data...\n",
      "[nltk_data]   Package abc is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /aiffel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('abc')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e874d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝~\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import abc\n",
    "corpus = abc.sents()\n",
    "print(\"슝~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7fae5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['PM', 'denies', 'knowledge', 'of', 'AWB', 'kickbacks', 'The', 'Prime', 'Minister', 'has', 'denied', 'he', 'knew', 'AWB', 'was', 'paying', 'kickbacks', 'to', 'Iraq', 'despite', 'writing', 'to', 'the', 'wheat', 'exporter', 'asking', 'to', 'be', 'kept', 'fully', 'informed', 'on', 'Iraq', 'wheat', 'sales', '.'], ['Letters', 'from', 'John', 'Howard', 'and', 'Deputy', 'Prime', 'Minister', 'Mark', 'Vaile', 'to', 'AWB', 'have', 'been', 'released', 'by', 'the', 'Cole', 'inquiry', 'into', 'the', 'oil', 'for', 'food', 'program', '.'], ['In', 'one', 'of', 'the', 'letters', 'Mr', 'Howard', 'asks', 'AWB', 'managing', 'director', 'Andrew', 'Lindberg', 'to', 'remain', 'in', 'close', 'contact', 'with', 'the', 'Government', 'on', 'Iraq', 'wheat', 'sales', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c97f977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "코퍼스의 크기 : 29059\n"
     ]
    }
   ],
   "source": [
    "print('코퍼스의 크기 :',len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4db73feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 학습 완료!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(sentences = corpus, vector_size = 100, window = 5, min_count = 5, workers = 4, sg = 0)\n",
    "print(\"모델 학습 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "996f6d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.9233262538909912), ('skull', 0.9110112190246582), ('Bang', 0.9056515097618103), ('asteroid', 0.9051857590675354), ('third', 0.9020100235939026), ('baby', 0.8994172811508179), ('dog', 0.8985921740531921), ('bought', 0.8975201845169067), ('rally', 0.8912350535392761), ('disc', 0.8889064192771912)]\n"
     ]
    }
   ],
   "source": [
    "model_result = model.wv.most_similar(\"man\")\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b6612ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델  load 완료!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model.wv.save_word2vec_format('~/aiffel/word_embedding/w2v') \n",
    "loaded_model = KeyedVectors.load_word2vec_format(\"~/aiffel/word_embedding/w2v\")\n",
    "print(\"모델  load 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68b9fcf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.9233262538909912), ('skull', 0.9110112190246582), ('Bang', 0.9056515097618103), ('asteroid', 0.9051857590675354), ('third', 0.9020100235939026), ('baby', 0.8994172811508179), ('dog', 0.8985921740531921), ('bought', 0.8975201845169067), ('rally', 0.8912350535392761), ('disc', 0.8889064192771912)]\n"
     ]
    }
   ],
   "source": [
    "model_result = loaded_model.most_similar(\"man\")\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e652f6",
   "metadata": {},
   "source": [
    "##### Word2Vec의 OOV 문제\n",
    "- 사전에 없는 단어에 대해서 Word2Vec은 임베딩 벡터값을 얻을 수 없음\n",
    "  - 해당 단어가 존재하지 않는 경우\n",
    "  - 오타를 낸 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b96911",
   "metadata": {},
   "source": [
    "#### 17-09. 임베딩 벡터의 시각화\n",
    "- 구글이 공개한 임베딩 벡터의 시각화 오픈소스인 임베딩 프로젝터(embedding projector) 를 사용\n",
    "- 임베딩 프로젝터를 통해서 어떤 임베딩 벡터들이 가까운 거리에 군집이 되어 있고,  \n",
    "  특정 임베딩 벡터와 유클리드 거리나 코사인 유사도가 높은지 확인\n",
    "- 참고 사이트\n",
    "  - https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b024c52",
   "metadata": {},
   "source": [
    "##### 임베딩 프로젝터에 tsv 파일 업로드하기\n",
    "- 참고 사이트\n",
    "  - https://projector.tensorflow.org/\n",
    "- 진행과정\n",
    "  - Step1에는 각각의 벡터값이 저장된 tsv 파일을 업로드\n",
    "  - Step2에는 메타 데이터의 tsv 파일을 업로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7604305",
   "metadata": {},
   "source": [
    "#### 17-10 FastText\n",
    "- 페이스북에서 개발한 FastText는 Word2Vec 이후에 등장한 워드 임베딩 방법\n",
    "- 문자 단위 n-gram(character-level n-gram) 표현을 학습한다\n",
    "- FastText는 단어 내부의 내부 단어(subwords)들을 학습한다는 아이디어"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ee3c64",
   "metadata": {},
   "source": [
    "##### FastText의 학습 방법\n",
    "- FastText도 Word2Vec과 마찬가지로 네거티브 샘플링을 사용하여 학습\n",
    "- \"(중심 단어, 주변 단어)\"의 쌍을 가지고 이 쌍이 포지티브인지 네거티브인지 예측을 진행\n",
    "- Word2Vec과 다른 점은 학습 과정에서 중심 단어에 속한 문자 단위 n-gram 단어 벡터들을 모두 업데이트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff251c3",
   "metadata": {},
   "source": [
    "##### OOV와 오타에 대한 대응\n",
    "- FastText는 Word2Vec과 달리 OOV와 오타에 강건하다(robust) 는 특징\n",
    "- 이는 단어장에 없는 단어라도, 해당 단어의 n-gram이 다른 단어에 존재하면 이로부터 벡터값을 얻는다는 원리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cdc69a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText 학습 완료!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "fasttext_model = FastText(corpus, window=5, min_count=5, workers=4, sg=1)\n",
    "print(\"FastText 학습 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4dfd2633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('resolving', 0.9407715201377869),\n",
       " ('fluctuating', 0.9389446973800659),\n",
       " ('emptying', 0.9341438412666321),\n",
       " ('malting', 0.9323472380638123),\n",
       " ('shooting', 0.9312872290611267),\n",
       " ('extracting', 0.9308481216430664),\n",
       " ('overwhelming', 0.9308262467384338),\n",
       " ('mounting', 0.9300415515899658),\n",
       " ('debilitating', 0.9286931157112122),\n",
       " ('declining', 0.9267855882644653)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model.wv.most_similar('overacting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35b4c865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('memory', 0.9458794593811035),\n",
       " ('mechanisms', 0.8634454011917114),\n",
       " ('musical', 0.8620406985282898),\n",
       " ('mechanism', 0.860693097114563),\n",
       " ('basic', 0.8561760187149048),\n",
       " ('mechanical', 0.8556269407272339),\n",
       " ('imagine', 0.8453862071037292),\n",
       " ('technical', 0.8452961444854736),\n",
       " ('visual', 0.8408808708190918),\n",
       " ('intelligence', 0.839508056640625)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model.wv.most_similar('memoryy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edcafe0",
   "metadata": {},
   "source": [
    "##### 한국어에서의 FastText\n",
    "- 음절 단위 FastText\n",
    "- 자소 단위 FastText\n",
    "  - 단어에 대해서 초성, 중성, 종성을 분리한다고 하고, 종성이 존재하지 않는 경우에는 _라는 토큰을 대신 사용\n",
    "  - (참고) https://brunch.co.kr/@learning/8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec11ae3",
   "metadata": {},
   "source": [
    "#### 17-11. GloVe\n",
    "- 글로브(Global Vectors for Word Representation, GloVe) 는 2014년에 미국 스탠포드 대학에서 개발한 워드 임베딩 방법론\n",
    "- 워드 임베딩의 두 가지 접근 방법인 카운트 기반과 예측 기반 두 가지 방법을 모두 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6a4109",
   "metadata": {},
   "source": [
    "##### 잠재 의미 분석(LSA, Latent Semantic Analysis)\n",
    "- LSA를 요약하면 DTM에 특잇값 분해를 사용하여 잠재된 의미를 이끌어내는 방법론\n",
    "  - 차원 축소의 특성으로 인해 새로운 단어가 추가되면 다시 DTM을 만들어 새로 차원 축소를 해야 한다.\n",
    "  - 단어 벡터간 유사도를 계산하는 측면에서 Word2Vec보다 성능이 떨어진다.\n",
    "- Word2Vec은 인공 신경망이 예측한 값으로부터 실제 레이블과의 오차를 구하고, 손실 함수를 통해서 인공 신경망을 학습\n",
    "  - Glove 연구진은 Word2Vec의 경우에는 LSA보다 단어 벡터 간 유사도를 구하는 능력은 뛰어나지만,    \n",
    "    LSA처럼 코퍼스의 전체적인 통계 정보를 활용하지는 못한다는 점을 한계로 지적함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514896d9",
   "metadata": {},
   "source": [
    "##### 윈도우 기반 동시 등장 행렬(Window based Co-occurrence Matrix)\n",
    "- 윈도우 기반 동시 등장 행렬은 행과 열을 전체 단어장(vocabulary)의 단어들로 구성\n",
    "- 어떤 i 단어의 윈도우 크기(window Size) 내에서 k 단어가 등장한 횟수를 i행 k열에 기재한 행렬\n",
    "- 이러한 동시 등장 행렬은 전치(transpose)해도 동일한 행렬이 된다는 특징"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a334655",
   "metadata": {},
   "source": [
    "##### 동시 등장 확률(Co-occurrence Probability)\n",
    "- 동시 등장 확률 P(k∣i)는 동시 등장 행렬로부터 특정 단어 i의 전체 등장 횟수를 카운트하고,   \n",
    "  특정 단어 i가 등장했을 때 어떤 단어 k가 등장한 횟수를 카운트하여 계산한 조건부 확률\n",
    "- i를 중심 단어(center word), k를 주변 단어(context word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d15cd5c",
   "metadata": {},
   "source": [
    "##### GloVe의 손실 함수 설계하기\n",
    "- GloVe는 동시 등장 행렬로부터 계산된 동시 등장 확률을 이용해 손실 함수를 설명\n",
    "- 동시 등장 행렬을 사용하고 있으니 코퍼스의 전체적인 통계 정보를 활용하는 '카운트 기반'의 방법론이면서,   \n",
    "  손실 함수를 통해 모델을 학습시키므로 '예측 기반'의 방법론\n",
    "  - (요약) 중심 단어 벡터와 주변 단어 벡터의 내적이 전체 코퍼스에서의 동시 등장 빈도의 로그값이 되도록 만드는 것\n",
    "- 손실 함수를 이해하기 위해서 GloVe의 변수들을 다음과 같이 정의\n",
    "  - ${X}$ : 동시 등장 행렬 (Co-occurrence Matrix) \n",
    "  - $X_i = \\sum_j X_{ij}$: 동시 등장 행렬에서 $i$행의 값을 모두 더한 값  \n",
    "  - $X_i = \\sum_j X_{ij}$: 동시 등장 행렬에서 $i$행의 값을 모두 더한 값  \n",
    "  - $P_{ik} = P(k \\mid i) = \\frac{X_{ik}}{X_i}$: 중심 단어 $i$가 등장했을 때 주변 단어 $k$가 등장할 확률  \n",
    "    - 예: $P(\\text{solid} \\mid \\text{ice})$: 단어 \"ice\"가 등장했을 때 \"solid\"가 등장할 확률  \n",
    "  - $\\frac{P_{ik}}{P_{jk}}$: 확률비 (likelihood ratio)  \n",
    "    - 예: $\\frac{P(\\text{solid} \\mid \\text{ice})}{P(\\text{solid} \\mid \\text{steam})} = 8.9$  \n",
    "  - $\\mathbf{w}_i$: 중심 단어 $i$의 임베딩 벡터  \n",
    "  - $\\tilde{\\mathbf{w}}_k$: 주변 단어 $k$의 임베딩 벡터  \n",
    "  - $b_i$, $\\tilde{b}_j$: 중심/주변 단어의 편향 (bias) 값  \n",
    "  - $f(X_{ij})$: 등장 빈도에 따른 가중치 함수  \n",
    "    - 일반적으로 $f(x) = \\left( \\frac{x}{x_{\\max}} \\right)^\\alpha$ if $x < x_{\\max}$, else $1$  \n",
    "    - 하이퍼파라미터: $\\alpha \\in (0,1)$, 예: $\\alpha = 0.75$, $x_{\\max} = 100$  \n",
    "\n",
    "$$\n",
    "J = \\sum_{i,j=1}^{V} f(X_{ij}) \\left( \\mathbf{w}_i^\\top \\tilde{\\mathbf{w}}_j + b_i + \\tilde{b}_j - \\log X_{ij} \\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f1afd6",
   "metadata": {},
   "source": [
    "##### pre-trained GloVe 모델 실습\n",
    "- GloVe는 2014년에 개발되었고, 2015년에 1.2 버전이 나온 이후로는 관리되지 않고 있기 때문에    \n",
    "  최신 버전의 python에서는 GloVe를 설치하는 것이 불가능\n",
    "- GloVe는 Word2Vec과 같이 OOV 문제를 가지고 있어서 'memoryy'라는 단어는 인식하지 못함.   \n",
    "  또한 pre-trained GloVe 모델은 한글이나 알파벳 대문자가 포함된 데이터셋으로 학습하지 않았기 때문에    \n",
    "  알파벳 소문자만 인식한다는 사실에 유의!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7862fb27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cat', 0.9218004941940308),\n",
       " ('dogs', 0.8513158559799194),\n",
       " ('horse', 0.7907583713531494),\n",
       " ('puppy', 0.7754920721054077),\n",
       " ('pet', 0.7724708318710327),\n",
       " ('rabbit', 0.7720814347267151),\n",
       " ('pig', 0.7490062117576599),\n",
       " ('snake', 0.7399188876152039),\n",
       " ('baby', 0.7395570278167725),\n",
       " ('bite', 0.7387937307357788)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "glove_model = api.load(\"glove-wiki-gigaword-50\")  # glove vectors 다운로드\n",
    "glove_model.most_similar(\"dog\")  # 'dog'과 비슷한 단어 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "40b8bd4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('impudence', 0.7842012047767639),\n",
       " ('puerile', 0.7816032767295837),\n",
       " ('winningly', 0.7644237875938416),\n",
       " ('grossness', 0.7576098442077637),\n",
       " ('deconstructions', 0.748936653137207),\n",
       " ('over-the-top', 0.7460805773735046),\n",
       " ('buffoonery', 0.746045708656311),\n",
       " ('impetuosity', 0.7415392398834229),\n",
       " ('sophomoric', 0.736961841583252),\n",
       " ('zaniness', 0.7353197336196899)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model.most_similar('overacting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c4e1b6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove_model.most_similar('memoryy')\n",
    "# Glove모델도 오타는 해결 못함"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
